# -*- coding: UTF-8 -*-
"""
GPUç¯å¢ƒæ£€æµ‹è„šæœ¬
å¿«é€Ÿæ£€æŸ¥CUDAå’ŒGPUä¿¡æ¯
"""
import torch

def check_gpu_environment():
    """æ£€æŸ¥GPUç¯å¢ƒé…ç½®"""
    print("=" * 60)
    print("PyTorch & CUDA ç¯å¢ƒæ£€æµ‹")
    print("=" * 60)
    
    # PyTorchç‰ˆæœ¬
    print(f"\nã€PyTorchä¿¡æ¯ã€‘")
    print(f"  PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"  CUDAç¼–è¯‘ç‰ˆæœ¬: {torch.version.cuda if torch.version.cuda else 'æœªå®‰è£…'}")
    print(f"  cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else 'æœªå®‰è£…'}")
    
    # CUDAå¯ç”¨æ€§
    print(f"\nã€CUDAçŠ¶æ€ã€‘")
    cuda_available = torch.cuda.is_available()
    print(f"  CUDAå¯ç”¨: {'âœ… æ˜¯' if cuda_available else 'âŒ å¦'}")
    
    if not cuda_available:
        print("\nâš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°CUDAï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰")
        print("   è§£å†³æ–¹æ¡ˆ:")
        print("   1. æ£€æŸ¥æ˜¯å¦å®‰è£…äº†NVIDIAæ˜¾å¡é©±åŠ¨")
        print("   2. å®‰è£…CUDAç‰ˆæœ¬çš„PyTorch:")
        print("      pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
        return
    
    # GPUæ•°é‡å’Œä¿¡æ¯
    gpu_count = torch.cuda.device_count()
    print(f"  å¯ç”¨GPUæ•°é‡: {gpu_count}")
    
    print(f"\nã€GPUè¯¦ç»†ä¿¡æ¯ã€‘")
    for i in range(gpu_count):
        props = torch.cuda.get_device_properties(i)
        print(f"\n  GPU {i}: {torch.cuda.get_device_name(i)}")
        print(f"    è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
        print(f"    æ€»æ˜¾å­˜: {props.total_memory / 1024**3:.2f} GB")
        print(f"    å¤šå¤„ç†å™¨æ•°: {props.multi_processor_count}")
        
        # æ¶æ„è¯†åˆ«
        compute_cap = f"{props.major}.{props.minor}"
        if props.major == 6:
            arch = "Pascal (GTX 10ç³»åˆ—)"
        elif props.major == 7 and props.minor == 0:
            arch = "Volta (V100)"
        elif props.major == 7 and props.minor == 5:
            arch = "Turing (RTX 20ç³»åˆ—, T4)"
        elif props.major == 8 and props.minor == 0:
            arch = "Ampere (A100, A30)"
            print(f"    ğŸš€ æ¶æ„: {arch} - æ”¯æŒTF32åŠ é€Ÿ!")
        elif props.major == 8 and props.minor == 6:
            arch = "Ampere (RTX 30ç³»åˆ—, A10)"
            print(f"    æ¶æ„: {arch}")
        elif props.major == 8 and props.minor == 9:
            arch = "Ada Lovelace (RTX 40ç³»åˆ—)"
            print(f"    æ¶æ„: {arch}")
        elif props.major == 9:
            arch = "Hopper (H100)"
            print(f"    ğŸš€ æ¶æ„: {arch} - æœ€æ–°æ¶æ„!")
        else:
            arch = f"æœªçŸ¥æ¶æ„ (è®¡ç®—èƒ½åŠ› {compute_cap})"
            print(f"    æ¶æ„: {arch}")
        
        # æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
        torch.cuda.set_device(i)
        mem_allocated = torch.cuda.memory_allocated(i) / 1024**3
        mem_reserved = torch.cuda.memory_reserved(i) / 1024**3
        mem_free = (props.total_memory - torch.cuda.memory_reserved(i)) / 1024**3
        print(f"    å½“å‰æ˜¾å­˜ä½¿ç”¨: {mem_allocated:.2f} GB")
        print(f"    æ˜¾å­˜ä¿ç•™: {mem_reserved:.2f} GB")
        print(f"    æ˜¾å­˜å¯ç”¨: {mem_free:.2f} GB")
    
    # ç‰¹æ€§æ”¯æŒ
    print(f"\nã€é«˜çº§ç‰¹æ€§æ”¯æŒã€‘")
    
    # TF32æ”¯æŒï¼ˆä»…AmpereåŠä»¥ä¸Šï¼‰
    tf32_supported = torch.cuda.get_device_properties(0).major >= 8
    print(f"  TF32åŠ é€Ÿ: {'âœ… æ”¯æŒ' if tf32_supported else 'âŒ ä¸æ”¯æŒï¼ˆéœ€è¦Ampereæ¶æ„åŠä»¥ä¸Šï¼‰'}")
    if tf32_supported:
        print(f"    å½“å‰çŠ¶æ€: {'å·²å¯ç”¨' if torch.backends.cuda.matmul.allow_tf32 else 'æœªå¯ç”¨'}")
    
    # AMPæ”¯æŒ
    amp_supported = hasattr(torch.cuda.amp, 'autocast')
    print(f"  æ··åˆç²¾åº¦(AMP): {'âœ… æ”¯æŒ' if amp_supported else 'âŒ ä¸æ”¯æŒ'}")
    
    # cuDNN
    cudnn_enabled = torch.backends.cudnn.enabled
    print(f"  cuDNNåŠ é€Ÿ: {'âœ… å·²å¯ç”¨' if cudnn_enabled else 'âŒ æœªå¯ç”¨'}")
    print(f"  cuDNN benchmark: {'å·²å¯ç”¨' if torch.backends.cudnn.benchmark else 'æœªå¯ç”¨ï¼ˆå»ºè®®å¯ç”¨ï¼‰'}")
    
    # æ¨èé…ç½®
    print(f"\nã€è®­ç»ƒå»ºè®®ã€‘")
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    
    if "A100" in torch.cuda.get_device_name(0):
        print("  ğŸ¯ æ£€æµ‹åˆ°A100æ˜¾å¡ï¼")
        print("  å¼ºçƒˆå»ºè®®ä½¿ç”¨: captcha_train_a100_optimized.py")
        print(f"  æ¨èbatch_size: 128-256")
        print("  é¢„è®¡è®­ç»ƒé€Ÿåº¦: 150è½®çº¦50åˆ†é’Ÿ")
    elif "H100" in torch.cuda.get_device_name(0):
        print("  ğŸ¯ æ£€æµ‹åˆ°H100æ˜¾å¡ï¼")
        print("  å¼ºçƒˆå»ºè®®ä½¿ç”¨: captcha_train_a100_optimized.py")
        print(f"  æ¨èbatch_size: 256-512")
        print("  é¢„è®¡è®­ç»ƒé€Ÿåº¦: 150è½®çº¦30åˆ†é’Ÿ")
    elif total_memory >= 20:
        print(f"  æ£€æµ‹åˆ°å¤§æ˜¾å­˜GPU ({total_memory:.0f}GB)")
        print("  å»ºè®®ä½¿ç”¨: captcha_train_a100_optimized.py")
        print(f"  æ¨èbatch_size: {min(256, int(total_memory * 6))}")
    elif total_memory >= 10:
        print(f"  æ£€æµ‹åˆ°ä¸­ç­‰æ˜¾å­˜GPU ({total_memory:.0f}GB)")
        print("  å¯ä»¥ä½¿ç”¨æ ‡å‡†ç‰ˆ: captcha_train.py")
        print(f"  æ¨èbatch_size: 64-128")
    else:
        print(f"  æ£€æµ‹åˆ°å°æ˜¾å­˜GPU ({total_memory:.0f}GB)")
        print("  å»ºè®®ä½¿ç”¨æ ‡å‡†ç‰ˆ: captcha_train.py")
        print(f"  æ¨èbatch_size: 32-64")
    
    # æ€§èƒ½æµ‹è¯•
    print(f"\nã€æ€§èƒ½æµ‹è¯•ã€‘")
    print("  æ­£åœ¨è¿›è¡Œç®€å•æ€§èƒ½æµ‹è¯•...")
    
    device = torch.device("cuda:0")
    
    # çŸ©é˜µä¹˜æ³•æµ‹è¯•
    import time
    size = 2048
    a = torch.randn(size, size, device=device)
    b = torch.randn(size, size, device=device)
    
    # é¢„çƒ­
    for _ in range(3):
        c = torch.matmul(a, b)
    torch.cuda.synchronize()
    
    # æµ‹è¯•
    start = time.time()
    iterations = 20
    for _ in range(iterations):
        c = torch.matmul(a, b)
    torch.cuda.synchronize()
    elapsed = time.time() - start
    
    tflops = (2 * size**3 * iterations) / (elapsed * 1e12)
    print(f"  çŸ©é˜µä¹˜æ³•æ€§èƒ½: {tflops:.2f} TFLOPS")
    
    # å‚è€ƒæ€§èƒ½
    gpu_name = torch.cuda.get_device_name(0)
    if "A100" in gpu_name:
        print(f"  A100ç†è®ºå³°å€¼: ~19.5 TFLOPS (FP32), ~312 TFLOPS (TF32)")
        if tflops > 10:
            print(f"  æ€§èƒ½è¯„ä¼°: âœ… ä¼˜ç§€")
        elif tflops > 5:
            print(f"  æ€§èƒ½è¯„ä¼°: âš ï¸  è‰¯å¥½ï¼Œä½†å¯ä»¥æ›´å¥½")
        else:
            print(f"  æ€§èƒ½è¯„ä¼°: âš ï¸  åä½ï¼Œæ£€æŸ¥é©±åŠ¨å’ŒCUDAé…ç½®")
    
    print(f"\n{'=' * 60}")
    print("æ£€æµ‹å®Œæˆï¼")
    print("=" * 60)

if __name__ == '__main__':
    try:
        check_gpu_environment()
    except Exception as e:
        print(f"æ£€æµ‹è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
