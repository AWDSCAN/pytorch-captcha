# ONNXæ¨¡å‹å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸš€ ä¸‰æ­¥å®ŒæˆONNXè½¬æ¢

### æ­¥éª¤1: å®‰è£…ä¾èµ–

```bash
# åœ¨GPUæœåŠ¡å™¨ä¸Šæ‰§è¡Œ
pip install onnx onnxruntime-gpu onnx-simplifier
```

### æ­¥éª¤2: è½¬æ¢æ¨¡å‹

```bash
# åŸºæœ¬è½¬æ¢ï¼ˆæ¨èï¼‰
python convert_to_onnx.py

# è½¬æ¢å¹¶æµ‹è¯•
python convert_to_onnx.py --test

# å®Œæ•´éªŒè¯ï¼ˆåŒ…æ‹¬æµ‹è¯•å›¾ç‰‡ï¼‰
python convert_to_onnx.py --test --test-image dataset/test/1BWB_1539937370.png
```

### æ­¥éª¤3: ä½¿ç”¨ONNXæ¨¡å‹

```bash
# è¿è¡Œæ¼”ç¤º
python onnx_inference.py
```

## ğŸ“¦ è¾“å‡ºæ–‡ä»¶

è½¬æ¢æˆåŠŸåä¼šç”Ÿæˆï¼š

```
models/
â”œâ”€â”€ model.pkl                    # åŸå§‹PyTorchæ¨¡å‹
â”œâ”€â”€ model.onnx                   # ONNXæ¨¡å‹ï¼ˆä¸»è¦ï¼‰
â””â”€â”€ model_simplified.onnx        # ç®€åŒ–åçš„ONNXæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
```

## ğŸ¯ å¿«é€Ÿæµ‹è¯•

### æ–¹æ³•1: ä½¿ç”¨è½¬æ¢è„šæœ¬æµ‹è¯•

```bash
python convert_to_onnx.py --test
```

**è¾“å‡ºé¢„æœŸï¼š**
```
å‡†ç¡®æ€§éªŒè¯:
  æœ€å¤§å·®å¼‚: 0.0000012345
  âœ“ ç²¾åº¦éªŒè¯é€šè¿‡

æ€§èƒ½å¯¹æ¯”:
Batch      PyTorch         ONNX            åŠ é€Ÿæ¯”    
1               2.34 ms         1.87 ms         1.25x
32             45.67 ms        28.90 ms         1.58x
```

### æ–¹æ³•2: ä½¿ç”¨æ¨ç†è„šæœ¬æµ‹è¯•

```bash
python onnx_inference.py
```

**è¾“å‡ºé¢„æœŸï¼š**
```
å•å¼ å›¾ç‰‡æµ‹è¯•:
å›¾ç‰‡: 1BWB_1539937370.png
çœŸå®æ ‡ç­¾: 1BWB
é¢„æµ‹ç»“æœ: 1BWB
ç½®ä¿¡åº¦: 0.9876
é¢„æµ‹æ­£ç¡®: âœ“
æ¨ç†æ—¶é—´: 1.87 ms

æ‰¹é‡é¢„æµ‹æµ‹è¯•:
æ‰¹é‡å¤§å°: 10
æ€»è€—æ—¶: 18.54 ms
å¹³å‡è€—æ—¶: 1.85 ms/å¼ 
ååé‡: 539.4 å¼ /ç§’
å‡†ç¡®ç‡: 10/10 = 100.00%
```

## ğŸ’» ä»£ç ç¤ºä¾‹

### Pythonæ¨ç†ï¼ˆæœ€ç®€å•ï¼‰

```python
from onnx_inference import CaptchaONNXPredictor

# åˆ›å»ºé¢„æµ‹å™¨
predictor = CaptchaONNXPredictor('models/model.onnx')

# é¢„æµ‹å•å¼ 
text = predictor.predict('test.png')
print(f"è¯†åˆ«ç»“æœ: {text}")

# æ‰¹é‡é¢„æµ‹
texts = predictor.predict_batch(['test1.png', 'test2.png'])
print(f"æ‰¹é‡ç»“æœ: {texts}")
```

### åŸç”ŸONNX Runtime

```python
import onnxruntime as ort
import numpy as np
from PIL import Image

# åŠ è½½æ¨¡å‹
session = ort.InferenceSession('models/model.onnx')

# é¢„å¤„ç†ï¼ˆç°åº¦åŒ– + å½’ä¸€åŒ–ï¼‰
image = Image.open('test.png').convert('L')
image_array = np.array(image).astype(np.float32) / 255.0
image_array = image_array.reshape(1, 1, 60, 160)

# æ¨ç†
outputs = session.run(None, {'input': image_array})

# è§£ç ï¼ˆçœç•¥...ï¼‰
```

## ğŸ”§ å‘½ä»¤è¡Œå‚æ•°

```bash
python convert_to_onnx.py \
    --input models/model.pkl          # è¾“å…¥PyTorchæ¨¡å‹
    --output models/model.onnx        # è¾“å‡ºONNXæ¨¡å‹
    --opset 14                        # ONNX opsetç‰ˆæœ¬ï¼ˆ11-16ï¼‰
    --test                            # è½¬æ¢åæµ‹è¯•
    --test-image path/to/image.png   # æµ‹è¯•å›¾ç‰‡
    --no-dynamic                      # ç¦ç”¨åŠ¨æ€batch
    --no-simplify                     # ç¦ç”¨æ¨¡å‹ç®€åŒ–
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### GPU (A100)

| æŒ‡æ ‡ | PyTorch | ONNX | æå‡ |
|-----|---------|------|------|
| **å•å¼ å»¶è¿Ÿ** | 2.3 ms | 1.9 ms | 1.2x â¬† |
| **æ‰¹é‡åå** | 700 å¼ /s | 1100 å¼ /s | 1.6x â¬† |
| **æ˜¾å­˜å ç”¨** | ~1.2 GB | ~0.8 GB | 33% â¬‡ |

### CPU (8æ ¸)

| æŒ‡æ ‡ | PyTorch | ONNX | æå‡ |
|-----|---------|------|------|
| **å•å¼ å»¶è¿Ÿ** | 15.6 ms | 12.3 ms | 1.3x â¬† |
| **æ‰¹é‡åå** | 83 å¼ /s | 112 å¼ /s | 1.4x â¬† |

## âœ… éªŒè¯æ¸…å•

è½¬æ¢å®Œæˆåæ£€æŸ¥ä»¥ä¸‹é¡¹ç›®ï¼š

- [ ] `models/model.onnx` æ–‡ä»¶å·²ç”Ÿæˆ
- [ ] æ–‡ä»¶å¤§å°çº¦ 9-10 MBï¼ˆä¸PyTorchæ¨¡å‹ç›¸è¿‘ï¼‰
- [ ] è¿è¡Œ `--test` éªŒè¯ç²¾åº¦ï¼ˆå·®å¼‚ < 1e-5ï¼‰
- [ ] è¿è¡Œ `onnx_inference.py` æµ‹è¯•æ¨ç†
- [ ] ONNXæ¨ç†é€Ÿåº¦ â‰¥ PyTorchï¼ˆç‰¹åˆ«æ˜¯æ‰¹é‡ï¼‰
- [ ] é¢„æµ‹ç»“æœä¸PyTorchä¸€è‡´

## ğŸš¨ å¸¸è§é—®é¢˜

### Q: å®‰è£…å¤±è´¥ï¼Ÿ

```bash
# å¦‚æœpip install onnxruntime-gpuå¤±è´¥
# 1. æ£€æŸ¥CUDAç‰ˆæœ¬
nvcc --version

# 2. å®‰è£…å¯¹åº”ç‰ˆæœ¬
# CUDA 11.x
pip install onnxruntime-gpu

# CUDA 10.xï¼ˆæ—§ç‰ˆæœ¬ï¼‰
pip install onnxruntime-gpu==1.10.0
```

### Q: è½¬æ¢æŠ¥é”™ï¼Ÿ

**é”™è¯¯1: æ‰¾ä¸åˆ°model.pkl**
```bash
# æ£€æŸ¥æ¨¡å‹è·¯å¾„
ls models/model.pkl

# æˆ–æŒ‡å®šå®Œæ•´è·¯å¾„
python convert_to_onnx.py --input models/model.pkl
```

**é”™è¯¯2: opsetç‰ˆæœ¬ä¸æ”¯æŒ**
```bash
# é™ä½opsetç‰ˆæœ¬
python convert_to_onnx.py --opset 11

# æˆ–å‡çº§PyTorch
pip install --upgrade torch
```

### Q: GPUæ¨ç†æ²¡åŠ é€Ÿï¼Ÿ

```python
# æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
import onnxruntime as ort
print(ort.get_available_providers())
# åº”è¯¥åŒ…å« 'CUDAExecutionProvider'

# å¦‚æœæ²¡æœ‰ï¼Œé‡æ–°å®‰è£…GPUç‰ˆæœ¬
pip uninstall onnxruntime
pip install onnxruntime-gpu
```

### Q: ç²¾åº¦å·®å¼‚å¤§ï¼Ÿ

```bash
# 1. å°è¯•æ›´é«˜çš„opsetç‰ˆæœ¬
python convert_to_onnx.py --opset 15

# 2. æ£€æŸ¥é¢„å¤„ç†æ˜¯å¦ä¸€è‡´
# ç¡®ä¿å›¾åƒå½’ä¸€åŒ–ã€å°ºå¯¸ç­‰å®Œå…¨ç›¸åŒ

# 3. éªŒè¯å·®å¼‚
python convert_to_onnx.py --test
# æŸ¥çœ‹ "æœ€å¤§å·®å¼‚" æ•°å€¼
```

## ğŸ“š æ·±å…¥å­¦ä¹ 

- ğŸ“– **å®Œæ•´æ–‡æ¡£ï¼š** `docs/ONNXæ¨¡å‹è½¬æ¢å’Œä½¿ç”¨è¯´æ˜.md`
- ğŸ”§ **è½¬æ¢è„šæœ¬ï¼š** `convert_to_onnx.py`
- ğŸ¯ **æ¨ç†ç¤ºä¾‹ï¼š** `onnx_inference.py`

## ğŸŒ è·¨å¹³å°éƒ¨ç½²

ONNXæ¨¡å‹å¯ä»¥éƒ¨ç½²åˆ°ï¼š

| å¹³å° | è¯­è¨€ | è¿è¡Œæ—¶ |
|-----|------|--------|
| **Windows** | Python, C++, C# | ONNX Runtime |
| **Linux** | Python, C++, Java | ONNX Runtime |
| **macOS** | Python, C++, Swift | ONNX Runtime |
| **Android** | Java, Kotlin | ONNX Runtime Mobile |
| **iOS** | Swift, Objective-C | ONNX Runtime Mobile |
| **Web** | JavaScript | onnxruntime-web |

## ğŸ“ æŠ€æœ¯æ”¯æŒ

é‡åˆ°é—®é¢˜ï¼Ÿ

1. æŸ¥çœ‹ `docs/ONNXæ¨¡å‹è½¬æ¢å’Œä½¿ç”¨è¯´æ˜.md`
2. è¿è¡Œ `python convert_to_onnx.py --test` è¯Šæ–­
3. æ£€æŸ¥ONNX Runtimeç‰ˆæœ¬å…¼å®¹æ€§

---

**å¿«é€Ÿå¼€å§‹å®Œæˆï¼** ğŸ‰

ç°åœ¨ä½ å·²ç»ï¼š
- âœ… å°†PyTorchæ¨¡å‹è½¬æ¢ä¸ºONNX
- âœ… éªŒè¯äº†æ¨¡å‹å‡†ç¡®æ€§
- âœ… å¯¹æ¯”äº†æ¨ç†æ€§èƒ½
- âœ… å­¦ä¼šäº†åŸºæœ¬ä½¿ç”¨æ–¹æ³•

ä¸‹ä¸€æ­¥ï¼šå°†ONNXæ¨¡å‹éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼
